{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"clearfix\" style=\"padding: 10px; padding-left: 0px\">\n",
    "<a href=\"http://bombora.com\"><img src=\"https://app.box.com/shared/static/e0j9v1xjmubit0inthhgv3llwnoansjp.png\" width=\"200px\" class=\"pull-right\" style=\"display: inline-block; margin: 5px; vertical-align: middle;\"></a>\n",
    "<h1> Bombora Data Science: <br> *Interview Exam* </h1>\n",
    "</div>\n",
    "\n",
    "<img width=\"200px\" src=\"https://app.box.com/shared/static/15slg1mvjd1zldbg3xkj9picjkmhzpa5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Welcome\n",
    "\n",
    "Welcome! This notebook contains interview exam questions referenced in the *Instructions* section in the `README.md`—please read that first, *before* attempting to answer questions here.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\" style=\"margin: 10px\">\n",
    "<p style=\"font-weight:bold\">ADVICE</p>\n",
    "<p>*Do not* read these questions, and panic, *before* reading the instructions in `README.md`.</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n",
    "<p style=\"font-weight:bold\">WARNING</p>\n",
    "\n",
    "<p>If using <a href=\"https://try.jupyter.org\">try.jupyter.org</a> do not rely on the server for anything you want to last - your server will be <span style=\"font-weight:bold\">deleted after 10 minutes of inactivity</span>. Save often and rember download notebook when you step away (you can always re-upload and start again)!</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "## Have fun!\n",
    "\n",
    "Regardless of outcome, getting to know you is important. Give it your best shot and we'll look forward to following up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Algo + Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 1.1: Fibionacci\n",
    "![fib image](https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Fibonacci_spiral_34.svg/200px-Fibonacci_spiral_34.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.1\n",
    "Given $n$ where $n \\in \\mathbb{N}$ (i.e., $n$ is an integer and $n > 0$), write a function `fibonacci(n)` that computes the Fibonacci number $F_n$, where $F_n$ is defined by the recurrence relation:\n",
    "\n",
    "$$ F_n = F_{n-1} + F_{n-2}$$\n",
    "\n",
    "with initial conditions of:\n",
    "\n",
    "$$ F_1 = 1,  F_2 = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T07:12:05.384462Z",
     "start_time": "2020-09-08T07:12:05.381164Z"
    }
   },
   "outputs": [],
   "source": [
    "def fibonacci(n):\n",
    "    '''\n",
    "    1. Recursion\n",
    "    I will first solve this question in a recursion way. However, for this solution, both fibonacci(n-1) and fibonacci(n-2) need to compute its fibonacci number all the way. \n",
    "    That means to get fibonacci(n-1), it will need to compute fibonacci(n-2) and fibonacci(n-3). However, to get fibonacci(n), it will compute fibonacci(n-2) again.\n",
    "    It is time consuming. Some same computation steps are repeative.\n",
    "    '''\n",
    "    assert isinstance(n, int) and n > 0 # n is an integer and n > 0\n",
    "    \n",
    "    if n == 1 or n == 2:\n",
    "        return 1\n",
    "    return fibonacci(n-1) + fibonacci(n-2) # recursively getting fibonacci(n-1) and fibonacci(n-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.2\n",
    "What's the complexity of your implementation?\n",
    "\n",
    "1. Time Complexity\n",
    "\n",
    "Let T represent the time complexity,\n",
    "\n",
    "$T(n) = T(n-1) + T(n-2) < T(n-1) + T(n-1) = 2*T(n-1) = 2*2*T(n-2) = ... = O(2^n)$\n",
    "\n",
    "2. Space Complexity\n",
    "\n",
    "This recursion method can be written into a recursion tree. We know in order to get fibonacci(n), we need to extract fibonacci(n-1), fibonacci(n-2),..., and so on. Then, the length of this path is the depth of the tree, which is proportional to n. Each node requests O(1) to store the result. Therefore, the full path requests O(n)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.3\n",
    "Consider an alternative implementation to compute Fibonacci number $F_n$ and write a new function, `fibonacci2(n)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T07:12:05.453224Z",
     "start_time": "2020-09-08T07:12:05.449990Z"
    }
   },
   "outputs": [],
   "source": [
    "def fibonacci2(n):\n",
    "    '''\n",
    "    2. Loop + Two Pointers\n",
    "    For this solution, I will save the fibonacci(n-1) and fibonacci(n-2) as current and previous so that I decrease the time complexity to be O(n).\n",
    "    The current and previou flags are updated at the same step.\n",
    "    '''\n",
    "    assert isinstance(n, int) and n > 0 # n is an integer and n > 0\n",
    "    \n",
    "    current = previous = 1\n",
    "    while n-2 > 0:\n",
    "        # update the current and previous flag at the same time.\n",
    "        current, previous = current+previous, current\n",
    "        n -= 1\n",
    "    return current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.4\n",
    "What's the complexity of your implementation?\n",
    "\n",
    "1. Time Complexity\n",
    "\n",
    "The loop takes n-2 times, so the time complexity is O(n).\n",
    "\n",
    "2. Space Complexity\n",
    "\n",
    "These iterative method only requires memory space for the current and previous flag, which will be O(1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.5\n",
    "What are some examples of optimizations that could improve computational performance?\n",
    "\n",
    "**Dynamic programming** is a very good but sometimes hard to think of method to deal with this kind of problem. Basically, once we notice that fibonacci(n) is computed repeatedly , it's time to divide the whole problem and use looping to compute each stage. In order not ot repeatedly computing one thing, we will save the current answer for the next step.\n",
    "\n",
    "**Matrix** is another way and is used quite often in Machine Learning. By converting multiple vectors into a matrix, we can do computation in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 1.2: Linked List\n",
    "![ll img](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Singly-linked-list.svg/500px-Singly-linked-list.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.2.1\n",
    "Consider a [singly linked list](https://en.wikipedia.org/wiki/Linked_list), $L$. Write a function `is_palindrome(L)` that detects if $L$ is a [palindrome](https://en.wikipedia.org/wiki/Palindrome), by returning a bool, `True` or `False`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T07:12:05.587470Z",
     "start_time": "2020-09-08T07:12:05.583531Z"
    }
   },
   "outputs": [],
   "source": [
    "class Nodes:\n",
    "    def __init__(self, val=0, next=None):\n",
    "        self.val = val\n",
    "        self.next = None\n",
    "        \n",
    "def is_palindrome(L):\n",
    "    '''\n",
    "    1. I will traverse through the whole linked list and save all elements in a new list. Then, I will compare the new list and the reversed new list and see if they are the same.\n",
    "    \n",
    "    '''\n",
    "    new_list = []\n",
    "    head = L\n",
    "    while head is not None: # Iteratively extracting each element in the linked list and save them in new_list\n",
    "        new_list.append(head.val)\n",
    "        head = head.next\n",
    "    return new_list == new_list[::-1] # compare new_list with reversed new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T07:12:05.598559Z",
     "start_time": "2020-09-08T07:12:05.589528Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "new = Nodes(1)\n",
    "new.next = Nodes(2)\n",
    "new.next.next = Nodes(3)\n",
    "new.next.next.next = Nodes(2)\n",
    "new.next.next.next.next = Nodes(1)\n",
    "# new.next.next.next.next.next = Nodes(1)\n",
    "\n",
    "is_palindrome(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.2.2\n",
    "What is the complexity of your implementation?\n",
    "\n",
    "1. Time Complexity\n",
    "\n",
    "It requires O(n) for both traversing through the linked list and comparing the new list.\n",
    "\n",
    "2. Space Complexity\n",
    "\n",
    "O(n) for new_list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.2.3\n",
    "Consider an alternative implementation to detect if L is a palindrome and write a new function, `is_palindrome2(L)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T07:12:05.657164Z",
     "start_time": "2020-09-08T07:12:05.651981Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_palindrome2(L):\n",
    "    '''\n",
    "    2. For this solution, I would like to decrease the space complexity. To do so, I will set two pointers, slow and fast.\n",
    "       The fast pointer goes two steps ahead each time and the slow pointer goes only one step. That means when the fast one \n",
    "       reaches the end, the slow one reaches the mid node. Then, I will start to reverse the linked list from the position of the slow pointer.\n",
    "       After reversing the linked list, I can now check is_palindrome from the head and the end.\n",
    "    '''\n",
    "    slow = fast = L\n",
    "    while fast is not None and fast.next is not None: # I can find the mid point from the slow pointer when the fast pointer reaches the end.\n",
    "        slow = slow.next\n",
    "        fast = fast.next.next\n",
    "    # starting to reverse the half of the linked list from the position of the slow pointer\n",
    "    slow = reverse(slow)\n",
    "    fast = L\n",
    "    \n",
    "    # use slow and fast to check is_palindrome respectively from the end and the beginning\n",
    "    while slow is not None:\n",
    "        if slow.val != fast.val: # if the elements at slow and fast are different, return False\n",
    "            return False\n",
    "        slow = slow.next # if the elements are the same, try to compare the next elements.\n",
    "        fast = fast.next\n",
    "    return True\n",
    "        \n",
    "def reverse(node):\n",
    "    prev = None\n",
    "    while node is not None:\n",
    "        temp = node.next\n",
    "        node.next = prev\n",
    "        prev = node\n",
    "        node = temp\n",
    "    return prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T07:12:05.661965Z",
     "start_time": "2020-09-08T07:12:05.659061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_palindrome2(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.2.4\n",
    "What's the complexity of this implementation?\n",
    "\n",
    "1. Time Complexity\n",
    "\n",
    "This function still needs O(n) for traversing the linked list and making comparison.\n",
    "\n",
    "2. Space Complexity\n",
    "\n",
    "There is no extra memory needed for the new list in this method. Although I will still use 2 memory space for slow and fast pointers, the space complexity is now O(1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.2.5 \n",
    "What are some examples of optimizations that could improve computational performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prob + Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.1: Finding $\\pi$ in a random uniform?\n",
    "<img src=https://www.epicurus.com/food/recipes/wp-content/uploads/2015/03/Pi-Day.jpg width=\"480\">\n",
    "\n",
    "Given a uniform random generator $[0,1)$ (e.g., use your language's standard libary to generate random value), write a a function `compute_pi` to compute [$\\pi$](https://en.wikipedia.org/wiki/Pi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T15:23:50.913288Z",
     "start_time": "2020-09-04T15:23:50.910407Z"
    }
   },
   "source": [
    "Ans:\n",
    "\n",
    "This is a statistical simulation problem. Basically, I have a circle inside a square. What I want to compute is that if I drop n random points in the square, what percentage of them will be in the circle area. Notes: n is a large number.\n",
    "\n",
    "Assuming $r(radius)=1$, the circular area is $\\pi*(r^2)=pi$ and the square area is $2*2=4$.\n",
    "\n",
    "The ratio of circular area and the square area is $\\pi/4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T07:12:05.817698Z",
     "start_time": "2020-09-08T07:12:05.815608Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T07:12:05.823396Z",
     "start_time": "2020-09-08T07:12:05.819779Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_pi(n):\n",
    "    '''\n",
    "    My goal is to compute the probability or ratio by droping random data points.\n",
    "    ''' \n",
    "    in_circle = 0\n",
    "    for _ in range(n):\n",
    "        x = np.random.uniform()\n",
    "        y = np.random.uniform()\n",
    "        if x**2 + y**2 <= 1:\n",
    "            in_circle += 1\n",
    "    pi = (in_circle/n)*4\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-08T07:12:05.869520Z",
     "start_time": "2020-09-08T07:12:05.825294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1516\n"
     ]
    }
   ],
   "source": [
    "print(compute_pi(10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.2: Making a 6-side die roll a 7?\n",
    "\n",
    "Using a single 6-side die, how can you generate a random number between 1 - 7?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "First of all, I need to create a distribution in which each of the number among 1 to 7 has the same probability. If I roll the dice twice, they can generate 36 configurations. Then, I can try to make 1 to 7 represent 5 configurations. my suggestion is in the following.\n",
    "\n",
    "- When the dice is rolled to 1 at the first time and 1 as well at the second, I am required to roll the dice again twice. This represents the rest of 1 configuration which has the probability of $1/36$.\n",
    "\n",
    "- When the dice is rolled to 1 at the first time and 2\\~6 at the second, it is 7 and has the probability of $1/6 * 5/6 = 5/36$.\n",
    "\n",
    "- When the dice is rolled to 2\\~6 at the first time and any number at the second, I will let the second number represent the final answer. This probability of $5/6 * 1/6 = 5/36$ is for 1\\~6.\n",
    "\n",
    "Then, all of the number is generated uniformly and randomly with a probability of $5/36$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.3: Is normality uniform?\n",
    "\n",
    "<img src=https://rednaxela1618.files.wordpress.com/2014/06/uniformnormal.png width=\"480\">\n",
    "\n",
    "\n",
    "Given draws from a normal distribution with known parameters, how can you simulate draws from a uniform distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.4: Should you pay or should you go?\n",
    "\n",
    "![coin flip](https://lh5.ggpht.com/iwD6MnHeHVAXNBgrO7r4N9MQxxYi6wT9vb0Mqu905zTnNlBciONAA98BqafyjzC06Q=w300)\n",
    "\n",
    "Let’s say we play a game where I keep flipping a coin until I get heads. If the first time I get heads is on the nth coin, then I pay you $2^{(n-1)}$ US dollars. How much would you pay me to play this game? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.5: Uber vs. Lyft\n",
    "\n",
    "![uber vs lyft](http://usiaffinity.typepad.com/.a/6a01347fc1cb08970c01bb0876bcbe970d-pi)\n",
    "\n",
    "You request 2 UberX’s and 3 Lyfts. If the time that each takes to reach you is IID, what is the probability that all the Lyfts arrive first? What is the probability that all the UberX’s arrive first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.6: Pick your prize\n",
    "<img src=https://miro.medium.com/max/1100/1*m5b3O9sE68UCXjLw5oxy2g.png width=\"480\">\n",
    "\n",
    "A prize is placed at random behind one of three doors and you are asked to pick a door. To be concrete, say you always pick door 1. Now the game host chooses one of door 2 or 3, opens it and shows you that it is empty. They then give you the option to keep your picked door or switch to the unopened door. Should you stay or switch if you want to maximize your probability of winning the prize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Conceptual ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 3.1 Why study gradient boosting or neural networks?\n",
    "\n",
    "Consider a regression setting where $X \\in \\mathbb{R}^p$ and $Y \\in \\mathbb{R}$. The goal is to come up with a function $f(X): \\mathbb{R}^p \\rightarrow \\mathbb{R}$ that minimizes the squared-error loss $(Y - f(X))^2$. Since X, Y are random variables, we seek to minimize the expectation of the squared error loss as follows\n",
    "\\begin{equation}\n",
    "EPE(f) = \\mathbb{E}\\left[(Y-f(X)^2\\right]\n",
    "\\end{equation}\n",
    "where EPE stands for expected prediction error. One can show that minimizing the expected prediction error leads to the following _regression function_\n",
    "\\begin{equation}\n",
    "f(x) = \\mathbb{E}\\left[Y|X=x\\right]\n",
    "\\end{equation}\n",
    "\n",
    "The goal of any method is to approximate the regression function above, which we denote as $\\hat{f}(x)$. For example, linear regression explicitly assumes that the regression function is approximately linear in its arguments, i.e. $\\hat{f}(x) = x^T\\beta$ while a neural network provides a nonlinear approximation of the regression function. \n",
    "\n",
    "The simplest of all these methods is [k-nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). Given $x$ and some neighbourhood of $k$ points $N_k(x)$, $\\hat{f}(x)$ is simply the average of all $y_i|x_i \\in N_k(x)$.  Let $N$ denote the training sample size. Under mild regularity conditions on the joint probability distribution $Pr(X, Y)$, one can show that as $N \\rightarrow \\infty$, $k \\rightarrow \\infty$ such that $k/N \\rightarrow 0$, then $\\hat{f}(x) \\rightarrow f(x)$ where $\\rightarrow$ means approaches or goes to. In other words, the k-nearest neighbors algorithm converges to the ideal solution as both the training sample size and number of neighbors increase to infinity.\n",
    "\n",
    "Now given this _universal approximator_, why look any further and research other methods? Please share your thoughts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 3.2 Model Selection and Assesment\n",
    "\n",
    "Consider a multiclass classification problem with a large number of features $p >> N$, for e.g $p=10000, N=100$ The task is threefold\n",
    "1. Find a \"good\" subset of features that show strong _univariate_ correlation with class labels\n",
    "2. Using the \"good\" subset, build a multi class classifier\n",
    "3. Estimate the generalization error of the final model\n",
    "\n",
    "Given this dataset, outline your approach and please be sure to cover the following\n",
    "- Data splitting\n",
    "- Model Selection: either estimating the performance of different classifiers or the same classifier with different hyperparameters\n",
    "- Model Assessment: having chosen a classifier, estimating the generalization error\n",
    "\n",
    "Assume all features are numerical, the dataset contains no NULLS, outliers, etc. and doesn't require any preprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "1. Data Splitting\n",
    "\n",
    "Before splitting the full dataset, I will first take a look at the distribution of the target variable in case that there exists the imbalanced dataset problem. Considering the size of the dataset is pretty small, first of all, I will randomly split the full dataset into two subsets, training and testing sets, with a percentage of 80% and 20%. In the training set, I will do 10-fold cross validation so that it will further be randomly splitted into 10 training-validation subsets. At this step, some upsampling or undersampling methods may be applied on the training set if the imbalanced problem exists.\n",
    "\n",
    "2. Dimension Reduction\n",
    "\n",
    "Based on the assumption of this dataset, the feature size is way bigger than the sample size. Curse of dimenionality, which not only leads to high computation cost but also over-fitting problems, would happen here. There are some techniques I can try.\n",
    "\n",
    " - No feature selection: This could cause overfitting.\n",
    " - Filter method: Some features with high linear correlation to each other are redundant. A part of them can be removed. Also, this specific method also measures linear relationship.\n",
    " - Greedy: Backward and forward feature selection methods will take a high computation cost. I usually don't use them.\n",
    " - Regularization and tree-based models: These methods are directly connected with the models. For example, we can use decision trees on our numerical features to get feature importance. Besides, l1 and l2 regularization with logistic regression, SVM, and multilayer perceptron are common methods to reduce overfitting.\n",
    " - Dimension reduction techniques: Methods, like PCA, Multidimensional scaling, generate a new subset of features from the original feature sets.\n",
    " \n",
    "The methods above should be deeply explored in this hign-dimension dataset.\n",
    "\n",
    "3. Model Selection\n",
    "\n",
    "I would set up the pipeline starting from dimension reduction and model training. For model training, grid search and 10-fold cross validation for tuning hyperparameters are applied here. I will first try a wide range of hyperparameters based on grid search and then focus on a limited range by observing the metrics from 10-fold cross validation. Multiple models will be tested to see which one works better in this dataset.\n",
    "\n",
    "4. Model Assessment\n",
    "\n",
    "Since this is a classification problem, the metrics for regression models, such as MSE and MAE, are not suitable. Instead, accuracy, ROC, and f1-score should be applied. However, accuracy can be misleading according to imbalanced class distribution. Therefore, I would prefer to use ROC as the metrics when implementing cross validation.\n",
    "\n",
    "Model reliability and generalization ability are important during model assessment, I will observe learning rate for training loss and validation loss when training the model to avoid overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
