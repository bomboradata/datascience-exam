{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"clearfix\" style=\"padding: 10px; padding-left: 0px\">\n",
    "<a href=\"http://bombora.com\"><img src=\"https://app.box.com/shared/static/e0j9v1xjmubit0inthhgv3llwnoansjp.png\" width=\"200px\" class=\"pull-right\" style=\"display: inline-block; margin: 5px; vertical-align: middle;\"></a>\n",
    "<h1> Bombora Data Science: <br> *Interview Exam* </h1>\n",
    "</div>\n",
    "\n",
    "<img width=\"200px\" src=\"https://app.box.com/shared/static/15slg1mvjd1zldbg3xkj9picjkmhzpa5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Welcome\n",
    "\n",
    "Welcome! This notebook contains interview exam questions referenced in the *Instructions* section in the `README.md`â€”please read that first, *before* attempting to answer questions here.\n",
    "\n",
    "<div class=\"alert alert-info\" role=\"alert\" style=\"margin: 10px\">\n",
    "<p style=\"font-weight:bold\">ADVICE</p>\n",
    "<p>*Do not* read these questions, and panic, *before* reading the instructions in `README.md`.</p>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\" style=\"margin: 10px\">\n",
    "<p style=\"font-weight:bold\">WARNING</p>\n",
    "\n",
    "<p>If using <a href=\"https://try.jupyter.org\">try.jupyter.org</a> do not rely on the server for anything you want to last - your server will be <span style=\"font-weight:bold\">deleted after 10 minutes of inactivity</span>. Save often and rember download notebook when you step away (you can always re-upload and start again)!</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "## Have fun!\n",
    "\n",
    "Regardless of outcome, getting to know you is important. Give it your best shot and we'll look forward to following up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exam Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Algo + Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 1.1: Fibionacci\n",
    "![fib image](https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Fibonacci_spiral_34.svg/200px-Fibonacci_spiral_34.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.1\n",
    "Given $n$ where $n \\in \\mathbb{N}$ (i.e., $n$ is an integer and $n > 0$), write a function `fibonacci(n)` that computes the Fibonacci number $F_n$, where $F_n$ is defined by the recurrence relation:\n",
    "\n",
    "$$ F_n = F_{n-1} + F_{n-2}$$\n",
    "\n",
    "with initial conditions of:\n",
    "\n",
    "$$ F_1 = 1,  F_2 = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the number of terms 15\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "n = int(input(\"Enter the number of terms \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "8\n",
      "13\n",
      "21\n",
      "34\n",
      "55\n",
      "89\n",
      "144\n",
      "233\n",
      "377\n",
      "610\n"
     ]
    }
   ],
   "source": [
    "## Iterative implementation\n",
    "def fibonacci(n):\n",
    "    if n < 0:\n",
    "        print('Please enter a valid n')\n",
    "    else:\n",
    "        first, second = 1, 1\n",
    "        for i in range(n):\n",
    "            first, second = second, first + second\n",
    "    return first\n",
    "\n",
    "for i in range(n):\n",
    "    print(fibonacci(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.2\n",
    "What's the complexity of your implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complexity of the above implementation is O(n) since we are running the for loop almost n times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.3\n",
    "Consider an alternative implementation to compute Fibonacci number $F_n$ and write a new function, `fibonacci2(n)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "8\n",
      "13\n",
      "21\n",
      "34\n",
      "55\n",
      "89\n",
      "144\n",
      "233\n",
      "377\n",
      "610\n"
     ]
    }
   ],
   "source": [
    "## Recursive implementation \n",
    "def fibonacci2(n):\n",
    "    if n <=1:  \n",
    "        return 1  \n",
    "    else:  \n",
    "        return(fibonacci2(n-1) + fibonacci2(n-2))  \n",
    "\n",
    "if n <= 0:\n",
    "    print('Please enter a valid value for n') \n",
    "else:  \n",
    "    for i in range(n):\n",
    "        print(fibonacci2(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.4\n",
    "What's the complexity of your implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complexity of this implementation is approximately $O(2^n)$ since for every function call we are calling the function 2 more times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.1.5\n",
    "What are some examples of optimizations that could improve computational performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we could try memoization - we basically increase space complexity and create an array of n elements.  We store the output of every function call in this array. In this way, for the next element we only need to look at the last two elements of the array instead of calling the function again. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 1.2: Linked List\n",
    "![ll img](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6d/Singly-linked-list.svg/500px-Singly-linked-list.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.2.1\n",
    "Consider a [singly linked list](https://en.wikipedia.org/wiki/Linked_list), $L$. Write a function `is_palindrome(L)` that detects if $L$ is a [palindrome](https://en.wikipedia.org/wiki/Palindrome), by returning a bool, `True` or `False`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.2.2\n",
    "What is the complexity of your implementation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.2.3\n",
    "Consider an alternative implementation to detect if L is a palindrome and write a new function, `is_palindrome2(L)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.2.4\n",
    "What's the complexity of this implementation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q 1.2.5 \n",
    "What are some examples of optimizations that could improve computational performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prob + Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.1: Finding $\\pi$ in a random uniform?\n",
    "<img src=https://www.epicurus.com/food/recipes/wp-content/uploads/2015/03/Pi-Day.jpg width=\"480\">\n",
    "\n",
    "Given a uniform random generator $[0,1)$ (e.g., use your language's standard libary to generate random value), write a a function `compute_pi` to compute [$\\pi$](https://en.wikipedia.org/wiki/Pi)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.2: Making a 6-side die roll a 7?\n",
    "\n",
    "Using a single 6-side die, how can you generate a random number between 1 - 7?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.3: Is normality uniform?\n",
    "\n",
    "<img src=https://rednaxela1618.files.wordpress.com/2014/06/uniformnormal.png width=\"480\">\n",
    "\n",
    "\n",
    "Given draws from a normal distribution with known parameters, how can you simulate draws from a uniform distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this by using inverse transform sampling. \n",
    "\n",
    "Let us assume that we model the random variable $A$ as the given normal distribution. If we know that the draw from the normal distribution is $A = a$, we want to check if we can find it's equivalent point that can be simulated from the uniform distribution. \n",
    "\n",
    "To simulate draws from a uniform distribution given a normal distribution, we want to find a sample $(x)$ from the uniform distribution such that the CDF of the uniform distribution at the point $x$ is equal to the CDF of the random variable $A$ at point $a$ from the normal distribution.\n",
    "\n",
    "Mathematically we write this as\n",
    "\n",
    "$$ P(A \\leq a) = P(X \\leq x) $$ \n",
    "\n",
    "where $X$ is a random variable drawn from the uniform distribution and $A$ is a random variable drawn from the normal distribution.\n",
    "\n",
    "Given $a$, we want to find $x$.\n",
    "\n",
    "Let $\\phi$ be the CDF function of a normal distribution. \n",
    "\n",
    "$$ P(A \\leq a) = \\phi (a) $$\n",
    "\n",
    "Since the given uniform distribution is uniform from $[0,1)$, its pdf is given by the constant 1. (total area under the PDF curve intergrates to 1). \n",
    "\n",
    "Therefore, CDF at point $x$ is given by - \n",
    "\n",
    "$$ P(X \\leq x) = x \\times 1 = x $$\n",
    "\n",
    "Since the CDFs are equal, \n",
    "\n",
    "$$ x = \\phi(a) $$\n",
    "\n",
    "Hence, given a sample from the above normal distribution, we can simulate a draw from a uniform distribution based on the sample's cumulative probability. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.4: Should you pay or should you go?\n",
    "\n",
    "![coin flip](https://lh5.ggpht.com/iwD6MnHeHVAXNBgrO7r4N9MQxxYi6wT9vb0Mqu905zTnNlBciONAA98BqafyjzC06Q=w300)\n",
    "\n",
    "Letâ€™s say we play a game where I keep flipping a coin until I get heads. If the first time I get heads is on the nth coin, then I pay you $2^{(n-1)}$ US dollars. How much would you pay me to play this game? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.5: Uber vs. Lyft\n",
    "\n",
    "![uber vs lyft](http://usiaffinity.typepad.com/.a/6a01347fc1cb08970c01bb0876bcbe970d-pi)\n",
    "\n",
    "You request 2 UberXâ€™s and 3 Lyfts. If the time that each takes to reach you is IID, what is the probability that all the Lyfts arrive first? What is the probability that all the UberXâ€™s arrive first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.6: Pick your prize\n",
    "<img src=https://miro.medium.com/max/1100/1*m5b3O9sE68UCXjLw5oxy2g.png width=\"480\">\n",
    "\n",
    "A prize is placed at random behind one of three doors and you are asked to pick a door. To be concrete, say you always pick door 1. Now the game host chooses one of door 2 or 3, opens it and shows you that it is empty. They then give you the option to keep your picked door or switch to the unopened door. Should you stay or switch if you want to maximize your probability of winning the prize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Conceptual ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 3.1 Why study gradient boosting or neural networks?\n",
    "\n",
    "Consider a regression setting where $X \\in \\mathbb{R}^p$ and $Y \\in \\mathbb{R}$. The goal is to come up with a function $f(X): \\mathbb{R}^p \\rightarrow \\mathbb{R}$ that minimizes the squared-error loss $(Y - f(X))^2$. Since X, Y are random variables, we seek to minimize the expectation of the squared error loss as follows\n",
    "\\begin{equation}\n",
    "EPE(f) = \\mathbb{E}\\left[(Y-f(X)^2\\right]\n",
    "\\end{equation}\n",
    "where EPE stands for expected prediction error. One can show that minimizing the expected prediction error leads to the following _regression function_\n",
    "\\begin{equation}\n",
    "f(x) = \\mathbb{E}\\left[Y|X=x\\right]\n",
    "\\end{equation}\n",
    "\n",
    "The goal of any method is to approximate the regression function above, which we denote as $\\hat{f}(x)$. For example, linear regression explicitly assumes that the regression function is approximately linear in its arguments, i.e. $\\hat{f}(x) = x^T\\beta$ while a neural network provides a nonlinear approximation of the regression function. \n",
    "\n",
    "The simplest of all these methods is [k-nearest neighbors](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). Given $x$ and some neighbourhood of $k$ points $N_k(x)$, $\\hat{f}(x)$ is simply the average of all $y_i|x_i \\in N_k(x)$.  Let $N$ denote the training sample size. Under mild regularity conditions on the joint probability distribution $Pr(X, Y)$, one can show that as $N \\rightarrow \\infty$, $k \\rightarrow \\infty$ such that $k/N \\rightarrow 0$, then $\\hat{f}(x) \\rightarrow f(x)$ where $\\rightarrow$ means approaches or goes to. In other words, the k-nearest neighbors algorithm converges to the ideal solution as both the training sample size and number of neighbors increase to infinity.\n",
    "\n",
    "Now given this _universal approximator_, why look any further and research other methods? Please share your thoughts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 3.2 Model Selection and Assesment\n",
    "\n",
    "Consider a multiclass classification problem with a large number of features $p >> N$, for e.g $p=10000, N=100$ The task is threefold\n",
    "1. Find a \"good\" subset of features that show strong _univariate_ correlation with class labels\n",
    "2. Using the \"good\" subset, build a multi class classifier\n",
    "3. Estimate the generalization error of the final model\n",
    "\n",
    "Given this dataset, outline your approach and please be sure to cover the following\n",
    "- Data splitting\n",
    "- Model Selection: either estimating the performance of different classifiers or the same classifier with different hyperparameters\n",
    "- Model Assessment: having chosen a classifier, estimating the generalization error\n",
    "\n",
    "Assume all features are numerical, the dataset contains no NULLS, outliers, etc. and doesn't require any preprocessing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To find a good subset of features, we need to identify which are the features that are independently strongly correlated with the target variable. \n",
    "\n",
    "First, we need to identify what do we mean by \"good\". In our subset, do we take all features that are statistically significant in their correlation with class labels or do we decide a heuristic number 'k' that gives us the top 'k' features that are most strongly correlated with class labels. If we want the first and since our features are numerical whereas the target variable is categorical, we can use ANOVA to calculate statistical significance between our features and target. If we want the top 'k' features, we can use mutual information to get a value for how correlated each feature is with the target and then pick the top 'k' features with the highest mutual information.  \n",
    "\n",
    "2. Since we have a small sample size (N = 100), we want to avoid using complex models that can overfit our data. We want to select a model that gives us the best generalized performance, is robust with small datasets, is preferably interpretable, and between multiple choices we want to choose the simplest model. \n",
    "\n",
    "To start with, we can use either a logistic regression or decision tree to build a multi-class classifier baseline. However, since we can have a large number of features even after feature reduction, a decision tree is likely to overfit our data when we have a small sample size coupled with a large number of features (unless we choose k features where k << 100).Â However, we can get an idea about the relative importance of the selected features through a decision tree. \n",
    "\n",
    "A logistic regression (LR) is an ideal choice to start with (since this is multi-class we can use the 'one-vs-rest' scheme). While a logistic regression model tends to have a higher bias as compared to boosted tree models but it also has a lower variance. When sample size is small, and especially if it is combined with more features, a LR is likely to give the best performance based on the bias-variance trade-off. This can be tested as well by methods I have described further. We can also compare performance across other classifiers like a linear-SVM, decision trees, random forests, boosting methods etc. to see if the above assumptions are valid for the given dataset. In each of these we will have to slightly tweak our models to have them work for multi-class classification. \n",
    "\n",
    "Given these constraints, we select a few options from classifiers to choose from and use holdout evaluation methods to evaluate the relative performance of each of these classifiers. Within each classifier, we can modify its hyperparameters to identify the right point of bias-variance tradeoff at which we can expect the best generalized performance for each classifier. \n",
    "\n",
    "3. A standard way of evaluating and comparing the performance of a classifier (including at different hyperparameter settings) is to create a train-validation-test split of our data. While creating this split, we ensure that each split is randomly sampled from our dataset and there is no overlap of instances across any of these 3 splits. \n",
    "\n",
    "For each model+hyperparameter selection, we train our classifiers on the train dataset and then compare their performance (based on an appropriate evaluation metric) on a validation dataset. The loss function which we minimize upon in training need not be the same as the metric we evaluate against in validation. However, it is important that we test each model+hyperparameter combination on the same validation dataset. Since this is a classification problem, we can find the optimal classifier based on comparing AUC and other metrics obtained from the confusion matrix. The particular metric can be selected based on what our aim out of this classification is. Based on this we can give higher importance to precision(when false positives are more expensive) or recall(when false negatives are more expensive). We can also take a combination of the two through observing the F1 metric. \n",
    "\n",
    "Since we have a small dataset, we can use cross-validation to train and validate our data on different samples of our train+validation dataset. This helps us compute the variance in our estimates and identify the model combination that on average gives us the best performance. We can also consider upsampling our data if needed to further aid model selection. We select the model+hyperparameter combination that gives us the best performance on our validation dataset. We then retrain the model on our train+validation dataset as on adding more data, we can expect the most optimal classifier to give us an even safer and more robust output. \n",
    "\n",
    "After identifying and retraining the model+hyperparameter combination, we can test this combination on a test dataset to see how well our classifier generalizes over unseen data. Generally the train-validation-test split is 70-20-10 but this is more of a heuristic split and can be modified. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
